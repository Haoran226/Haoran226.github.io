<html class="gr__richzhang_github_io"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><script src="./The Unreasonable Effectiveness of Deep Networks as a Perceptual Metric_files/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}
	h1 {
		font-weight:300;
	}

	.disclaimerbox {
		background-color: #eee;
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}

	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}

	img.rounded {
		border: 0px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}

	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}

	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}

	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		        15px 15px 0 0px #fff, /* The fourth layer */
		        15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		        20px 20px 0 0px #fff, /* The fifth layer */
		        20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		        25px 25px 0 0px #fff, /* The fifth layer */
		        25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}

	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}

	.vert-cent {
		position: relative;
	    top: 50%;
	    transform: translateY(-50%);
	}

	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>
		<title>USTNet: Unsupervised Shape-to-Shape Translation via Disentangled Representations</title>

  </head>

  <body data-gr-c-s-loaded="true">
    <br>
          <center>
          	<span style="font-size:34px">USTNet: Unsupervised Shape-to-Shape Translation via Disentangled Representations</span><br>

	  		  <br>

	  		  <table align="center" width="900px">
	  			  <tbody><tr>
	  	              <td align="center" width="150px">
	  					<center>
	  						<span style="font-size:22px"><a href="https://haoran226.github.io/USTNet" target="_blank" >Haoran Wang</a><sup>1</sup></span>
		  		  		</center>
		  		  	  </td>
	  	              <td align="center" width="150px">
	  					<center>
	  						<span style="font-size:22px"><a href="https://haoran226.github.io/USTNet"  target="_blank" > Jiaxin Li</a><sup>1</sup></span>
		  		  		</center>
		  		  	  </td>
	  	              <td align="center" width="200px">
	  					<center>
	  						<span style="font-size:22px"><a href="http://www.staff.science.uu.nl/~telea001/" target="_blank" >Alexandru Telea</a><sup>2</sup></span>
		  		  		</center>
		  		  	  </td>
	  	              <td align="center" width="200px">
	  					<center>
	  						<span style="font-size:22px"><a href="https://www.cs.rug.nl/~jiri/index.html" target="_blank"> Jiří Kosinka</a><sup>3</sup></span>
		  		  		</center>
		  		  	  </td>
	  	              <td align="center" width="200px">
	  					<center>
	  						<span style="font-size:22px"><a href="http://zizhao.me/" target="_blank">Zizhao Wu</a><sup>1</sup></span>
		  		  		</center>
		  		  	  </td>
	  			  </tr>
			  </tbody></table>

			  <table align="center" width="800px"><tbody>
			  <tr>
	  	              <td align="center" width="50px"></td>
	  	              <td align="center" width="400px">
	  					<center>
				          	<span style="font-size:18px"><sup>1</sup>Hangzhou Dianzi University, Hangzhou, China</span>
		  		  		</center>
		  	      </td>
		  	      </td>
	  	              <td align="center" width="50px"></td>
			  </tr>
			  </tbody></table>

			  <table align="center" width="800px"><tbody>
			  <tr>
	  	              <td align="center" width="50px"></td>
	  	              <td align="center" width="400px">
	  					<center>
				          	<span style="font-size:18px"><sup>2</sup>Utrecht University, Utrecht, the Netherlands</span>
		  		  		</center>
		  	      </td>
                  </td>
	  	              <td align="center" width="50px"></td>
			  </tr>
			  </tbody></table>

			  <table align="center" width="800px"><tbody>
			  <tr>
	  	              <td align="center" width=800px">
	  					<center>
							  <span style="font-size:18px"><sup>3</sup>University of Groningen, Groningen, the Netherlands</span>
		  		  		</center>
		  	      </td>
			  </tr>
			  </tbody></table>

			  <br>

	  		  <table align="center" width="1100px">
	  			  <tbody><tr>
	  	              <td align="center" width="275px">
	  					<center>
				          	<span style="font-size:18px"></span>
		  		  		</center>
		  		  	  </td>
	  	              <td align="center" width="225px">
	  					<center>
	  						<span style="font-size:22px">Code <a href="https://github.com/Haoran226/USTNet" target="_blank" > [GitHub]</a></span>
		  		  		</center>
		  		  	  </td>
	  	              <td align="center" width="225px">
	  					<center>
							  <span style="font-size:22px">PG 2022<a href="https://haoran226.github.io/USTNet" target="_blank" > [Paper]</a></span>
	  						  <!-- <span style="font-size:22px">ICCV 2019 [Paper]</span> -->

		  		  		</center>
		  		  	  </td>
	  	              <td align="center" width="275px">
	  					<center>
				          	<span style="font-size:18px"></span>
		  		  		</center>
		  		  	  </td>
			  </tr></tbody></table>
          </center>

          <br>
  		  <table align="center" width="1100px">
  			  <tbody><tr>
  	              <td width="400px">
  					<center>
  	                	<img class="rounded" src="./figures/Figure1.png" width="1000px">
  	                	<br>
					</center>
  	              </td>
  	              </tr>
  	              </tbody></table>

  		  <br>
		  <hr>

  		  <center><h1>Abstract</h1></center>
			We propose USTNet, a novel deep learning approach designed for learning shape-to-shape translation from unpaired domains in an unsupervised manner. The core of our approach lies in disentangled representation learning that factors out the discriminative features of 3D shapes into content and style codes. Given input shapes from multiple domains, USTNet disentangles their representation into style codes that contain distinctive traits across domains and content codes that contain domain-invariant traits. By fusing the style and content codes of the target and source shapes, our method enables us to synthesize new shapes that resemble the target style and retain the content features of source shapes. Based on the shared style space, our method facilitates shape interpolation by manipulating the style attributes from different domains. Furthermore, by extending the basic building blocks of our network from two-class to multi-class classification, we adapt USTNet to tackle multi-domain shape-to-shape translation. Experimental results show that our approach can generate realistic and natural translated shapes and that our method leads to improved quantitative evaluation metric results compared to 3DSNet.
		<br><br><hr>

		<center><h1>Overview</h1></center>
  		    <table align="center" width="900px">
  			<tbody>
  			  	<tr>
  			  		<td align="center"><img class="round" style="width:1000px" src="./figures/Figure2.png"></td>
			  	</tr>
			  	<!-- <tr>
			  		<td align="center"><h3>Pipeline of EC-Net</h3></td>
			  	</tr> -->
			</tbody>
			</table>

			<br>



<!-- Global site tag (gtag.js) - Google Analytics -->



</body></html>
